<meta charset="utf-8">

<p>The Research Computing environment provides several types of
  storage. Understainging these storage options, how they impact job
  execution, and how they may impact other running jobs is essential
  when running in the Research Computing environment.</p>

<h2>Home</h2>

<p>Each user has a home directory available from all Research
  Computing nodes at <code>/home/${USER}/</code>. Each home directory
  is a limited to 2GB to prevent the use of the home directory as a
  target for job output, software installs, or other data likely to be
  used during a compute job. Home directories are not stored on a
  high-performance filesystem and, as such, they are not intended to
  be written to by compute jobs. <strong><em>Use of a home directory
  during a high-performance or parallel compute job may negatively
  affect the environment for all users.</em></strong></p>

<p>A <code>.snapshot/</code> directory is present in each home
  directory, and contains recent copies of the files at hourly,
  nightly and weekly intervals. These snapshots can be used to recover
  files after accidental deletion or corruption.</p>

<p>Home directories are protected by parity in a RAID storage system
  and are replicated at two geo-redundant locations.</p>

<h2 id="projects">Projects</h2>

<p>Each user has access to a 256GB project directory available from
  all Research Computing nodes at <code>/projects/${USER}/</code>. The
  project directory is intended to store software builds and smaller
  data sets. Like home directories, project directories are not
  intended to be written to by compute jobs. <strong><em>Significant
  use of a project directory during a high-performance or parallel
  compute job may negatively affect the environment for all
  users.</em></strong></p>

<p>A <code>.snapshot/</code> directory is present in each project
  directory, and contains recent copies of the files at hourly and
  nightly intervals. These snapshots can be used to recover files
  after accidental deletion or corruption.</p>

<p>Project directories are protected by parity in a RAID storage
  system and are replicated at two geo-redundant locations.</p>

<h2>Local scratch</h2>

<p>Local scratch directores are assigned automatically on compute
  nodes during job execution. Because these directories are assigned
  and removed automatically, the location of the directory is assigned
  to the <code>$SLURM_SCRATCH</code> environment variable.</p>

<p>No administrative limits are placed on the amount of data that can
  be placed in a temporary local scratch directory. Scratch
  directories are limited only by the physical capacity of their
  storage. However, some nodes create local scratch directories using
  an in-memory <code>tmpfs</code> filesystem. In this case, use of the
  scratch directory reduces the system memory available for the
  job.</p>

<p><strong><em>Local scratch directories are not backed up or
      checkpointed, and are not apropriate for long-term
      storage. Local scratch directories, and all data contained in
      them, are removed automatically when the running job
      ends.</em></strong></p>

<table>
  <caption>Implementation of local scratch directories</caption>

  <thead>
    <tr>
      <th>Partition</th>
      <th>Description</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>Janus</td>
      <td>12 GB in-memory <code>tmpfs</code> filesystem. Because
        Janus nodes have no <code>swap</code> space, scratch files
        directly compete with job processes for system memory.</td>
    </tr>
    <tr>
      <td>Himem</td>
      <td>10 TB local RAID filesystem.</td>
    </tr>
    <tr>
      <td>Serial</td>
      <td>1.8 TB local RAID filesystem.</td>
    </tr>
  </tbody>
</table>

<h2>General-purpose global scratch</h2>

<p>A central general-purpose scratch filesystem is available
  at <code>/rc_scratch/$USER/</code>. This space is intended for short
  term/parallel storage over the length of a job run.</p>

<p>No administrative limits are placed on the amount of data that can
  be placed in a general-purpose global scratch directory.</p>

<p>Janus compute nodes should use the high-performance Lustre scratch
  filesystem, not the general-purpose scratch filesystem. As such, the
  general-purpose scratch filesystem is mounted read-only on Janus
  compute nodes.</p>

<p><strong><em>General-purpose scratch directories are not backed up
      or checkpointed, and are not apropriate for long-term
      storage. Data may be purged at any time. Files are automatically
      removed 90 days after their initial creation
      (<code>ctime</code>).</em></strong> You can check whether you
  have files older than 90 days using <code>find</code>.</p>

<pre><code>$ find /rc_scratch/$USER/ -type f -ctime +90</code></pre>

<h2 id="janus-scratch">Janus scratch</h2>

<p>A central high-performance scratch Lustre filesystem is available
  at <code>/lustre/janus_scratch/$USER/</code>.</p>

<p>No administrative limits are placed on the amount of data that can
  be placed in a high-performance global scratch directory.</p>

<p>Non-Janus compute nodes have access to the high-performance Lustre
  scratch filesystem, but are not connected via a high-performance
  network. Local scratch directories or the general-purpose scratch
  filesystem are likely to provide better performance on non-Janus
  nodes.</p>

<p><strong><em>High-performance scratch directories are not backed up
      or checkpointed, and are not apropriate for long-term
      storage. Data may be purged at any time. Files are automatically
      removed 180 days after their initial creation
      (<code>ctime</code>).</em></strong> You can check whether you
  have files older than 180 days using <code>lfs find</code>.</p>

<pre><code>$ lfs find /lustre/janus_scratch/$USER/ -type f -ctime +180</code></pre>

<p>Janus Lustre is served by a DDN EXAScaler appliance running Lustre
  version 2.1.6.</p>

<h3>Metadata storage (<code>MDS</code>/<code>MDT</code>)</h3>

<ul>
  <li>2 Dell R710 MDS servers with QDR InfiniBand links to the Janus
    InfiniBand network</li>
  <li>DDN EF-3010 metadata disk controller with two 4Gb FibreChannel
    links to each Lustre metadata server (MDS)</li>
  <li>12 15k RPM SAS disks aggregated into a RAID 10 virtual metadata
    disk</li>
</ul>

<h3>Object storage (<code>OSS</code>/<code>OST</code>)</h3>

<ul>
  <li>8 Dell R710 OSS servers with QDR InfiniBand links to the Janus
    InfiniBand network</li>
  <li>4 DDN SFA10000 disk controllers each with two QDR InfiniBand
    links to the Lustre object storage servers (OSS)</li>
  <li>10 DDN SS6000 disk enclosures attached to the SFA10000 disk
  controllers</li>
  <li>600 2TB SATA disks (60 in each of the SS6000 disk enclosures)
    aggregated into 60 RAID-6 virtual disks</li>
</ul>

<div class="tutorial">Tutorial: <a href="../examples-and-tutorials/parallel-io-on-janus-lustre.html">Parallel
    IO on Janus Lustre</a></div>

<h2>The PetaLibrary</h2>

<p>The PetaLibrary is a National Science Foundation-subsidized service
  for the storage, archival, and sharing of research data, housed in
  the Space Sciences data center at CU-Boulder. It is available for a
  modest fee to any US-based researcher affiliated with the University
  of Colorado Boulder.</p>

<p>The PetaLibrary stores and archives research data, broadly defined
  as the scholarly output from researchers at the University of
  Colorado, as well as digital archives and special collections. The
  PetaLibrary is not an appropriate storage target for administrative
  data, data that is copyrighted by someone other than the project
  owner or members, or sensitive data (e.g., HIPAA, FERPA, ITAR, or
  classified).</p>

<p>PetaLibrary storage is allocated to discrete projects, analogous to
  an allocation of computational services units to a computational
  allocation. A PetaLibrary project is purchased and overseen by a
  Principal Investigator (PI), but may be made accessible to multiple
  additional members. A PI may have multiple PetaLibrary projects,
  each with a different storage allocation and list of authoized
  members. Each project may also have a Point of Contact (POC) person
  who is allowed to request changes to the project on behalf of the
  PI.</p>

<p>Each project is presented as a unique path on a posix-compliant
  filesystem.</p>

<p>The PetaLibrary system provides two primary services: PetaLibrary
  Active and PetaLibrary Archive.</p>

<p>The PetaLibrary has a 10 Gb/s connection to the CU Science Network,
  allowing transfers at speeds up to 800 MB/s.</p>

<h3>PetaLibrary Active</h3>

<p>The PetaLibrary Active storage service is mounted
  at <code>/work/</code> on all Research Computing computational
  systems. This storage may be used by compute workloads, but it is
  not designed to be performant under I/O-intensive applications or
  parallel writes. (Parallel or concurrent IO should
  target <a href="#janus-scratch">Janus scratch</a> in stead.</p>

<h3>PetaLibrary Archive and hierarchical storage management (HSM)</h3>

<p>PetaLibrary Archive is a higherarchical storage system that
  dynamically and simultaneously manages data on both disk and tape
  storage systems while presenting all data as part of a single,
  unified filesystem namespace. The process by which files are
  automatically migrated between disk and tape stoarge is called
  "hierarchical storage management" (HSM). Normally, recently-used or
  small files are kept on low-latency disk storage, while older or
  larger files are migrated to bulk tape storage. This provides good
  performance when accessing frequently-used data while remaining
  cost-effective for storing large quantities of archive data.</p>

<p>PetaLibrary Archive provides an optional "additional copy on tape"
  to protect against tape media failure. (All disk storage is
  protected from media failure by parity in a disk array.)</p>

<h3>Sharing data</h3>

<p>Data in the PetaLibrary can be shared with collaborators who do not
  possess a CU Identikey or RC account via
  the <a href="https://www.globus.org/data-sharing">Globus sharing
  service</a>. To enable sharing, a member of the PetaLibrary project
  must authenticate to the colorado#gridftp Globus endpoint and
  configure a "Globus shared endpoint" that references the intended
  PetaLibrary storage area. You can then grant access (read and/or
  write) to the shared endpoint to anyone with a Globus account.</p>

<p>A Globus Plus subscription is required to use the Globus sharing
  service. One Globus Plus subscription is provided with each
  PetaLibrary project.</p>

<p>To comply with NSF requirements, external collaborators should have
  a US-based affiliation.</p>

<p>For more information, contact us
  at <a href="mailto:rc-help@colorado.edu">rc-help@colorado.edu</a>.</p>
