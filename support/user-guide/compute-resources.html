<meta charset="utf-8">

<h1>Compute resources</h1>

<h2 id="janus">Janus</h2>

<p>The Janus supercomputer is a Linux compute cluster capable of 184
  TFLOPS (trillion floating-point operations per second). Funding for
  Janus was provided by the <a href="http://www.nsf.gov">National
  Science Foundation</a> (MRI
  grant <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=0821794">CNS-0821794</a>),
  the <a href="http://www.colorado.edu">University of Colorado
  Boulder</a>, and the <a href="https://ncar.ucar.edu">National Center
  for Atmospheric Research</a>.</p>

<table>
  <tr>
    <th>Operating system</th>
    <td><a href="http://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red
        Hat Enterprise Linux</a> 6</td>
  </tr>
  <tr>
    <th>Compute nodes</th>
    <td>1,368 total in 342 quad-node chassis</td>
  </tr>
  <tr>
    <th>Compute cores</th>
    <td>16,416 total,
      two <a href="http://ark.intel.com/products/47921/Intel-Xeon-Processor-X5660-12M-Cache-2_80-GHz-6_40-GTs-Intel-QPI">Intel
      Xeon X5660</a> (6*2.8GHz "Westmere") processors per node</td>
  </tr>
  <tr>
    <th>System memory</th>
    <td>32 TiB total, 24 GiB per node, 2 GiB per
      core<sup><a href="#fn1" id="ref1">1</a></sup></td>
  </tr>
  <tr>
    <th>Parallel storage</th>
    <td>800 TB high-performance Lustre scratch filesystem</td>
  <tr>
    <th>Primary interconnect</th>
    <td><a href="http://www.mellanox.com">Mellanox</a> QDR (40 Gbps)
      InfiniBand in a non-blocking fat-tree topology</td>
  </tr>
  <tr>
    <th>Ethernet</th>
    <td>Dell PowerConnect 10 GbE with connectivity to the CU Science
      Network</td>
  </tr>
</table>

<ol>
  <li id="fn1">The system image consumes about 4 GiB per node, leaving
    about 20 GiB free.<a href="#ref1">↩</a></li>
</ol>

<p>Access to Janus requires a <a href="quick-start.html">Research
    Computing account</a> and a supporting PI or faculty member who
    agrees to abide by the relevant export control requirements.</p>

<h2 id="crestone">Crestone</h2>

<p>Crestone is
  a <a href="http://www.dell.com/us/business/p/poweredge-m1000e/pd">Dell
  PowerEdge M1000e Blade system</a> and is provided for jobs requiring
  more memory or longer runtimes than are available on Janus. This
  system is intended for single-node jobs and does not have access to
  a high-speed, low-latency network interconnect.</p>

<table>
  <tr>
    <th>Operating system</th>
    <td><a href="http://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red
        Hat Enterprise Linux</a> 6</td>
  </tr>
  <tr>
    <th>Compute nodes</th>
    <td>16</td>
  </tr>
  <tr>
    <th>Compute cores</th>
    <td>192 total,
      two <a href="http://ark.intel.com/products/47921/Intel-Xeon-Processor-X5660-12M-Cache-2_80-GHz-6_40-GTs-Intel-QPI">Intel
      Xeon X5660</a> (6x2.8 GHz "Westmere") processors per node,
      <a href="http://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading/hyper-threading-technology.html">Hyper-Threading</a>
      enabled</td>
  </tr>
  <tr>
    <th>System memory</th>
    <td>1,536 GiB total, 96 GiB per node</td>
  </tr>
  <tr>
    <th>Local storage</th>
    <td>2 TB hard disk per node</td>
  </tr>
</table>

<p>Crestone nodes are accessed via the <code>crestone</code> QOS.</p>

<h2 id="blanca">Blanca</h2>

<p>The Research Computing Condo Computing service offers researchers
  the opportunity to purchase and own compute nodes that will be
  operated as part of a cluster, named “Blanca.” The aggregate cluster
  is made available to all condo partners while maintaining priority
  for the owner of each node.</p>

<p>Benefits to partners include</p>

<ul>
  <li>Data center rack space – including redundant power and cooling –
    is provided, as is scratch disk space.</li>
  <li>Partners get significantly prioritized access on nodes that they
    own and can run jobs on any nodes that are not currently in use by
    other partners.</li>
  <li>System configuration and administration, as well as technical
    support for users, is provided by RC staff.</li>
  <li>A standard software stack appropriate for a range of research
    needs is provided.  Partners are able to install additional
    software applications as needed.</li>
  <li>Bulk discount pricing is available for all compute node
  purchases.</li>
</ul>
 
<p>Three types of compute nodes are available. The available node
  types is expected to be refreshed each year.</p>

<table>
  <thead>
    <tr>
      <th>Node type</th>
      <th>Specifications</th>
      <th>Cost</th>
      <th>Potential uses</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Compute node</th>
      <td>
        <ul>
          <li>2x 12-core 2.5 GHz Intel “Haswell” processors</li>
          <li>128 GB RAM at 2133 MHz</li>
          <li>1x 1 TB 7200 RPM hard drive</li>
          <li>10 gigabit/s Ethernet</li>
        </ul>
      </td>
      <td>$6,875/node</td>
      <td>general purpose high-performance computation that doesn’t
        require low-latency parallel communication between nodes</td>
    </tr>
    <tr>
      <th>GPU node</th>
      <td>
        <ul>
          <li>2x 12-core 2.5 GHz Intel “Haswell” processors</li>
          <li>128 GB RAM at 2133 MHz</li>
          <li>1x 1 TB 7200 RPM hard drive</li>
          <li>10 gigabit/s Ethernet</li>
          <li>2x NVIDIA K20M GPU coprocessors</li>
        </ul>
      </td>
      <td>$12,509/node</td>
      <td>molecular dynamics, image processing</td>
    </tr>
    <tr>
      <th>Himem</th>
      <td>
        <ul>
          <li>4x 12-core 3.0 GHz Intel “Ivy Bridge” processors</li>
          <li>1024 GB RAM at 1600 MHz</li>
          <li>10x 1 TB 7200 RPM hard drives in high-performance RAID configuration</li>
          <li>10 gigabit/s Ethernet</li>
        </ul>
      </td>
      <td>$34,511/node</td>
      <td>genetics/genomics applications</td>
    </tr>
  </tbody>
</table>

<h2 id="himem">Himem</h2>

<p>There are 3 high-memory ("himem") nodes available. These nodes are
  available for use by the general community, but priority is given to
  departments who contributed to their purchase (e.g., IBG and
  Biofrontiers).</p>

<table>
  <tr>
    <th>Operating system</th>
    <td colspan="2"><a href="http://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red
        Hat Enterprise Linux</a> 6</td>
  </tr>
  <tr>
    <th>Compute nodes</th>
    <td>1</td>
    <td>2</td>
  </tr>
  <tr>
    <th>Compute cores</th>
    <td>32,
    four <a href="http://ark.intel.com/products/46498/Intel-Xeon-Processor-X7550-18M-Cache-2_00-GHz-6_40-GTs-Intel-QPI">Intel
    Xeon X7550</a> (8*2GHz "Nehalem-EX") processors,
      <a href="http://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading/hyper-threading-technology.html">Hyper-Threading</a>
      enabled</td>
    <td>80 total,
      four <a href="http://ark.intel.com/products/53577/Intel-Xeon-Processor-E7-8867L-30M-Cache-2_13-GHz-6_40-GTs-Intel-QPI">Intel
      Xeon E7-8867L</a> (10*2.13GHz "Westmere-EX") processors per
      node,
      <a href="http://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading/hyper-threading-technology.html">Hyper-Threading</a>
      enabled</td>
  </tr>
  <tr>
    <th>System memory</th>
    <td>512 GiB</td>
    <td>2 TiB total, 1 TiB per node</td>
  </tr>
  <tr>
    <th>Local storage</th>
    <td>2 TB hard disk</td>
    <td>16 TB hard disk per node</td>
  </tr>
</table>

<p>High-memory nodes are accessed via the <code>himem</code> QOS.</p>

<h2 id="gpu">GPU</h2>

<p>Two GPU nodes support visualization and GPU-accelerated
  applications.</p>

<table>
  <tr>
    <th>Operating system</th>
    <td><a href="http://www.redhat.com/en/technologies/linux-platforms/enterprise-linux">Red
        Hat Enterprise Linux</a> 6</td>
  </tr>
  <tr>
    <th>Compute nodes</th>
    <td>2</td>
  </tr>
  <tr>
    <th>Compute cores</th>
    <td>24 total,
      two <a href="http://ark.intel.com/products/47921/Intel-Xeon-Processor-X5660-12M-Cache-2_80-GHz-6_40-GTs-Intel-QPI">Intel
      Xeon X5660</a> (6*2.8GHz "Westmere") processors per node,
      <a href="http://www.intel.com/content/www/us/en/architecture-and-technology/hyper-threading/hyper-threading-technology.html">Hyper-Threading</a>
      enabled</td>
  </tr>
  <tr>
    <th>GPU</th>
    <td><a href="http://www.nvidia.com/docs/IO/43395/NV_DS_Tesla_M2050_M2070_Apr10_LowRes.pdf">Nvidia
        Tesla M2070</a></td>
  </tr>
  <tr>
    <th>System memory</th>
    <td>256 GiB total, 128 GiB per node</td>
  </tr>
  <tr>
    <th>Local storage</th>
    <td>two 1 TB hard disks per node</td>
  </tr>
</table>

<p>GPU-accelerated nodes are accessed via the <code>gpu</code>
  QOS.</p>
