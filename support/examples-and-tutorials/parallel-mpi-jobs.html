<meta charset="utf-8">

<p>There are three different mechanisms by which MPI jobs may be
  dispatched using Slurm. The recommended mechanism
  uses <code><a href="http://www.schedmd.com/slurmdocs/srun.html">srun</a></code>
  to directly launch tasks and initialize inter-process
  communication. More information on other mechanisms is available in
  the Slurm <a href="http://slurm.schedmd.com/mpi_guide.html">MPI and
  UPC Users Guide</a>.</p>

<h2>MPI dispatch with <code>srun</code></h2>

<p>These are example parallel job scripts using current best
  practices. They each run a 24-task MPI test job on two Janus nodes
  with a ten-minute time limit.</p>

<p>More information about MPI is available in the documentation for
  <a href="../mpi-distributed-programming.html">MPI and distributed
    programming</a>.</p>

<h3>Intel MPI</h3>

<pre><code>#!/bin/bash

#SBATCH --job-name mpi_test
#SBATCH --qos janus
#SBATCH --nodes 2
#SBATCH --ntasks-per-node 12
#SBATCH --time 00:10:00
#SBATCH --output mpi_test.out

# the slurm module provides the srun command
module load slurm

module load intel/impi-13.0.0

srun mpi_test</code></pre>

<h3>OpenMPI</h3>

<pre><code>#!/bin/bash

#SBATCH --job-name mpi_test
#SBATCH --qos janus
#SBATCH --nodes 2
#SBATCH --ntasks-per-node 12
#SBATCH --time 00:10:00
#SBATCH --output mpi_test.out

# the slurm module provides the srun command
module load slurm

module load openmpi/1.8.3_intel-13.0.0

srun mpi_test</code></pre>

<h3>MPICH</h3>

<pre><code>#!/bin/bash

#SBATCH --job-name mpi_test
#SBATCH --qos janus
#SBATCH --nodes 2
#SBATCH --ntasks-per-node 12
#SBATCH --time 00:10:00
#SBATCH --output mpi_test.out

# the slurm module provides the srun command
module load slurm

module load mpich/mpich-3.1.2_intel-13.0.0

# PMI-2 must be specified explicitly when using MPICH
srun --mpi=pmi2 mpi_test</code></pre>

<h2>Multiple program, multiple data (MPMD)</h2>

<p>Slurm supports multi-program MPI through the use of
  the <code>--multi-prog</code> argument to <code>srun</code>. In this
  case, the executable supplied to <code>srun</code> is expected to be
  a configuration file that maps MPI ranks to executables and
  arguments.</p>

<pre><code># mpmd-example.conf

0-9 ./a.out
10-24 ./b.out</code></pre>

<p>Compared to the above SPMD examples, simply modify
  the <code>srun</code> command to run with the defined MPMD
  configuration.</p>

<pre><code>srun --multi-prog mpmd-example.conf</code></pre>

<p>More information is available in
  the <a href="http://www.schedmd.com/slurmdocs/srun.html"><code>srun</code>
  manpage</a>, under the heading "MULTIPLE PROGRAM CONFIGURATION."</p>

<h2>Custom task geometry</h2>
<p>
If your MPI program requires a custom task geometry you will need to redefine
the environment variable <code>SLURM_TASKS_PER_NODE</code> and use
<code>mpiexec</code> to execute you program.
</p>
<p>
In this case we request 4 nodes with 12 tasks per node giving a total of
48 tasks. However we want the root rank (<code>rank 0</code>) to run on
the first node by itself, then the next two nodes to run 12 tasks each
(<code>ranks 1-12</code>) and last node to run 6 tasks only
(<code>ranks 13-18</code>).
</p>
<pre>
<code>
#!/bin/bash

#SBATCH --job-name mpi_test
#SBATCH --qos janus
#SBATCH --nodes 4
#SBATCH --ntasks-per-node 12
#SBATCH --time 00:10:00
#SBATCH --output mpi_test.out

# the slurm module provides the srun command
module load slurm

module load intel/impi-13.0.0

export SLURM_TASKS_PER_NODE='1,12(x2),6'
mpiexec ./mpi_test
</code>
</pre>
